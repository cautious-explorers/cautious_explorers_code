{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b2d118a-8f91-4164-b0af-990d4bc190a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['5_1_PSW.R', 'draw', '3_sample_scientist.ipynb', '2_interplays.ipynb', '4_regression_result.ipynb', '.ipynb_checkpoints', '5_2_PSW_result.ipynb', '1_processed_data.ipynb']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import tqdm\n",
    "from collections import Counter\n",
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)\n",
    "import matplotlib\n",
    "import statistics \n",
    "import math\n",
    "import pickle\n",
    "import scipy.io as scio\n",
    "import json\n",
    "import os\n",
    "import datetime\n",
    "import sys\n",
    "\n",
    "def save_pkl(path,obj):\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(obj,f)\n",
    "        \n",
    "def load_pkl(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "print(os.listdir())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b95e12-2f5e-451e-9a0a-13e83d442988",
   "metadata": {},
   "source": [
    "# deal with source data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb0a430-9f54-49fa-8970-994baf00d296",
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_doi = []\n",
    "authors_name = []\n",
    "dates = []\n",
    "paper_type =[]\n",
    "for pairs in tqdm.tqdm(os.walk(\"/public/aps/raw_data/aps-dataset-metadata-2020/\", topdown=False)):\n",
    "    root = pairs[0]\n",
    "    files = pairs[2]\n",
    "    for name in files:\n",
    "        path = os.path.join(root, name)\n",
    "        author_name = []\n",
    "        with open(path, 'r') as f:   #读取当前目录的json文件并解码成python数据\n",
    "            data = json.load(f)  #每个论文的数据\n",
    "            \n",
    "            date_list = list(map(int,data['date'].split('-'))) ##用横杠分割字符串并储存为list_int\n",
    "            date = datetime.date(date_list[0],date_list[1],date_list[2])\n",
    "            dates.append(date)\n",
    "            \n",
    "            paper_doi.append(data['id'])\n",
    "            \n",
    "            if 'articleType' in data.keys():\n",
    "                paper_type.append(data['articleType'])\n",
    "            else:\n",
    "                paper_type.append(None)\n",
    "                \n",
    "            if 'authors' in data.keys():\n",
    "                for i in range(len(data['authors'])):\n",
    "                    author_name.append(data['authors'][i]['name'])\n",
    "            else: x##有些作者没有名字记载\n",
    "                author_name = None\n",
    "            authors_name.append(author_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07bbab9-e014-48b0-80c4-77798787a652",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_paper_data = pd.DataFrame({\n",
    "    'paperDoi':paper_doi,\n",
    "    'authorName':authors_name,\n",
    "    'date':dates,\n",
    "    'type':paper_type\n",
    "})\n",
    "save_pkl('./data/meta_paper_data.pkl', meta_paper_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b4aaf5-f8c9-48c0-a6c5-3908b49a9b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_paper_data.date.min(),meta_paper_data.date.max(),len(meta_paper_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef67f99-4617-4bf1-97a6-bec86673bcdd",
   "metadata": {},
   "source": [
    "# count five-year citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de07071c-9a4b-4bdc-b82c-4e47f12dbd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_paper_data = load_pkl('./meta_paper_data.pkl')\n",
    "doi_date = dict(zip(meta_paper_data.paperDoi,meta_paper_data.date))\n",
    "cit_pair = pd.read_csv('/public/aps/raw_data/aps-dataset-citations-2020.csv')\n",
    "cit_pair_with_time = pd.merge(cit_pair, meta_paper_data[['paperDoi','date']].rename(columns={'paperDoi':'citing_doi','date':'citing_pubdate'}), on='citing_doi', how='left').drop_duplicates()\n",
    "cit_pair_with_time = pd.merge(cit_pair_with_time, meta_paper_data[['paperDoi','date']].rename(columns={'paperDoi':'cited_doi','date':'cited_pubdate'}), on='cited_doi', how='left').drop_duplicates()\n",
    "cit_pair_with_time = cit_pair_with_time.dropna().reset_index(drop=True)# 引用论文没有记载时间的删掉"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f67d38f-fe29-4c1f-918a-096cbbbc88eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 统计n年内被引用，和所有年份被引用\n",
    "n = 5\n",
    "# 只需要遍历一遍就行了\n",
    "# 论文被谁引用了\n",
    "citation_dict = {}\n",
    "# 论文引用了谁\n",
    "reference_dict = {}\n",
    "\n",
    "for i in tqdm.tqdm(range(len(cit_pair_with_time))):\n",
    "    # 被引数据：5年内\n",
    "    cited_pub_i = cit_pair_with_time.cited_pubdate.iloc[i]\n",
    "    citing_pub_i = cit_pair_with_time.citing_pubdate.iloc[i]\n",
    "    delta = citing_pub_i-cited_pub_i\n",
    "    cited_doi_i = cit_pair_with_time.cited_doi.iloc[i]\n",
    "    citing_doi_i = cit_pair_with_time.citing_doi.iloc[i]\n",
    "    \n",
    "    #  如果有时间记载\n",
    "    if (delta>=datetime.timedelta(days=0))&(delta<=datetime.timedelta(days=n*365)): ##五年的另外一种表示 365*5\n",
    "        if cited_doi_i in citation_dict.keys():\n",
    "        ## 如果被引论文已经在被引字典里了\n",
    "            citation_dict[cited_doi_i].add(citing_doi_i)\n",
    "        else:\n",
    "        ## 如果被引论文还没有在字典里，第一条\n",
    "            citation_dict[cited_doi_i] = set([citing_doi_i])\n",
    "\n",
    "    # 引用数据，全部  \n",
    "    if citing_doi_i in reference_dict.keys():\n",
    "        reference_dict[citing_doi_i].add(cited_doi_i)\n",
    "    else:\n",
    "        reference_dict[citing_doi_i] = set([cited_doi_i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a273c0e-5bf4-48be-9fdc-c5b8a6dca42b",
   "metadata": {},
   "source": [
    "# assign citations to paper data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be849cb6-4543-4a77-add8-6f5b395fa015",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_paper_data = meta_paper_data.drop_duplicates(subset=['paperDoi', 'date'])\n",
    "### 如果论文不在citation列表里，代表没有引用数据记载，则引用为0\n",
    "citations = []\n",
    "references = []\n",
    "cit_count =[]\n",
    "ref_count = []\n",
    "for i in tqdm.tqdm(range(len(meta_paper_data))):\n",
    "    paperdoi = meta_paper_data.paperDoi.iloc[i]\n",
    "    if paperdoi in citation_dict.keys():\n",
    "        citations.append(citation_dict[paperdoi])\n",
    "        cit_count.append(len(citation_dict[paperdoi]))\n",
    "    else:\n",
    "        citations.append(set([]))\n",
    "        cit_count.append(0)\n",
    "    if paperdoi in reference_dict.keys():\n",
    "        references.append(reference_dict[paperdoi])\n",
    "        ref_count.append(len(reference_dict[paperdoi]))\n",
    "    else:\n",
    "        references.append(set([]))\n",
    "        ref_count.append(0)\n",
    "\n",
    "meta_paper_data_2 = meta_paper_data.copy()\n",
    "meta_paper_data_2['citations'] = citations\n",
    "meta_paper_data_2['citCount'] = cit_count\n",
    "meta_paper_data_2['references'] = references\n",
    "meta_paper_data_2['refCount'] = ref_count\n",
    "\n",
    "meta_paper_data_2 = meta_paper_data_2.drop_duplicates(subset=['paperDoi', 'date']).reset_index(drop = True)\n",
    "save_pkl('./data_meta_paper_data_2.pkl',meta_paper_data_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f536d7d-bef7-47d5-ba26-254017f86b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_data_2['logCit'] = [np.log(i+1) for i in meta_data_2['citCount']]\n",
    "meta_data_2['year'] = [i.year for i in meta_data_2['date']]\n",
    "plt.plot(meta_data_2.groupby('year').logCit.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1fb0b7-8700-48af-98e8-96b9bbf42537",
   "metadata": {},
   "source": [
    "# deal with genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebc1cc4-644d-4d8f-8b04-5ef71ad86318",
   "metadata": {},
   "outputs": [],
   "source": [
    "PACS = pd.read_csv('/public/aps/raw_data/PACS.txt', keep_default_na=False)\n",
    "def get_genre(name):\n",
    "    PACS_code = []\n",
    "    for i in tqdm.tqdm(range(len(PACS))):\n",
    "        p_code = re.match(r'((.*)([0-9]{4})(.*))', str(PACS[name].iloc[i]).replace('.','').replace(' ','').replace(':',''))\n",
    "        if p_code:\n",
    "            if p_code.group(1)[:2].isdigit():\n",
    "                PACS_code.append(p_code.group(1)[:6].replace('−','-').replace('–','-'))\n",
    "            else:\n",
    "                PACS_code.append(None)\n",
    "        else:\n",
    "            PACS_code.append(None)\n",
    "    return PACS_code\n",
    "\n",
    "genres_set = set(get_genre('PACS1') + get_genre('PACS2') + get_genre('PACS3') + get_genre('PACS4') + get_genre('PACS5'))\n",
    "\n",
    "PACS1 = get_genre('PACS1')\n",
    "PACS2 = get_genre('PACS2')\n",
    "PACS3 = get_genre('PACS3')\n",
    "PACS4 = get_genre('PACS4')\n",
    "PACS5 = get_genre('PACS5')\n",
    "\n",
    "genres_list = []\n",
    "for i in tqdm.tqdm(range(len(PACS))):\n",
    "    gl = [PACS1[i],PACS2[i],PACS3[i],PACS4[i],PACS5[i]]\n",
    "    while None in gl:\n",
    "        gl.remove(None)\n",
    "    genres_list.append(gl)\n",
    "    \n",
    "doi_cor_genre = dict(zip(PACS.DOI,genres_list))\n",
    "len(doi_cor_genre)\n",
    "doi_cor_genre['10.1103/PhysRevA.60.R2614'] = ['0365Bz', '4250Dv', '89701c']\n",
    "doi_cor_genre['10.1103/PhysRevB.66.104415'] = ['7570Pa', '71301h', '78202e']\n",
    "doi_cor_genre['10.1103/PhysRevE.65.026128'] = ['05202y', '04402b', '05901m']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae82a8a-2659-4513-b47e-52779a50db3c",
   "metadata": {},
   "source": [
    "# assgin PACS data to paper data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c49e2a8-1112-4038-ad97-94407f9c7481",
   "metadata": {},
   "outputs": [],
   "source": [
    "genres = []\n",
    "for i in tqdm.tqdm(range(len(meta_paper_data_2))):\n",
    "    if meta_paper_data_2.paperDoi.iloc[i] in doi_cor_genre.keys():\n",
    "        genres.append(doi_cor_genre[meta_paper_data_2.paperDoi.iloc[i]])\n",
    "    else:\n",
    "        genres.append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210a5a62-d19d-48ed-b6fc-35e19c7d19a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_paper_data_3 = meta_paper_data_2.copy()\n",
    "meta_paper_data_3['genres'] = genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5161c06d-f4ab-405c-8b68-1f8b845f3c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_paper_data_3 = meta_paper_data_3.dropna(subset=['authorName','date','paperDoi']).reset_index(drop=True)\n",
    "part = meta_paper_data_3[(meta_paper_data_3.date>=datetime.date(1976,1,1))&(meta_paper_data_3.date<=datetime.date(2015,12,31))]\n",
    "part.isnull().sum()/len(part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e848bb89-afe0-404a-9c96-6a04840ef2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_pkl('./data/meta_paper_data_3.pkl',meta_paper_data_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdcb5f3-a83d-4c6e-a527-91f0d2e85dfd",
   "metadata": {},
   "source": [
    "# name disambiguation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4034e20b-2acb-4116-9f0d-7193d8ae8300",
   "metadata": {},
   "source": [
    "## build data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc613597-49c9-41bb-9275-05c315ed0b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_doi = []\n",
    "authors_name = []\n",
    "dates = []\n",
    "paper_affs = []\n",
    "for pairs in tqdm.tqdm(os.walk(\"/public/aps/raw_data/aps-dataset-metadata-2020/\", topdown=False)):\n",
    "    root = pairs[0]\n",
    "    files = pairs[2]\n",
    "    for name in files:\n",
    "        path = os.path.join(root, name)\n",
    "        author_name = []\n",
    "        with open(path, 'r') as f:   #读取当前目录的json文件并解码成python数据\n",
    "            data = json.load(f)  #每个论文的数据\n",
    "            \n",
    "            date_list = list(map(int,data['date'].split('-'))) ##用横杠分割字符串并储存为list_int\n",
    "            date = datetime.date(date_list[0],date_list[1],date_list[2])\n",
    "            dates.append(date)\n",
    "            \n",
    "            paper_doi.append(data['id'])\n",
    "                \n",
    "            if 'authors' in data.keys():\n",
    "                for i in range(len(data['authors'])):\n",
    "                    if 'affiliationIds' in data['authors'][i].keys():\n",
    "                        author_name.append((data['authors'][i]['name'], data['authors'][i]['affiliationIds']))\n",
    "                    else:\n",
    "                        author_name.append((data['authors'][i]['name'], None))\n",
    "\n",
    "            else: ##有些作者没有名字记载\n",
    "                author_name = None\n",
    "            authors_name.append(author_name)\n",
    "            \n",
    "            if 'affiliations' in data.keys():\n",
    "                aff_dict = {}\n",
    "                for aff in data['affiliations']:\n",
    "                    aff_dict[aff['id']]=aff['name']\n",
    "                paper_affs.append(aff_dict)\n",
    "            else:\n",
    "                paper_affs.append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b887bb-1e28-43f9-adef-60d57abfeb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_data = pd.DataFrame({\n",
    "    'paperDoi':paper_doi,\n",
    "    'authorName':authors_name,\n",
    "    'date':dates,\n",
    "    'paperAff':paper_affs})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33110ee7-b677-45f3-af3f-0e4cd80f62c1",
   "metadata": {},
   "source": [
    "## processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a2d897-f420-455a-9b99-262b893e76c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_data = meta_data.dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02609738-642e-4dde-9f2d-7f1b8b82ad21",
   "metadata": {},
   "outputs": [],
   "source": [
    "aid = []\n",
    "author_name = []\n",
    "alter_name = []\n",
    "doi = []\n",
    "ref_set = []\n",
    "coauthor_set = []\n",
    "author_aff = []\n",
    "first_letter = []\n",
    "last_name = []\n",
    "name_split = []\n",
    "gid = []\n",
    "journal = []\n",
    "raw_name = []\n",
    "count = 0\n",
    "test = 0\n",
    "for i in tqdm.tqdm(range(len(meta_data))):\n",
    "# for i in tqdm.tqdm(range(2)):\n",
    "    p_doi = meta_data.paperDoi.iloc[i]\n",
    "    authors_info = meta_data.authorName.iloc[i]\n",
    "    #每篇文章的作者ids\n",
    "    pap_aids = [str(count+i) for i in range(len(authors_info))]\n",
    "    ## 机构对应名字的词典\n",
    "    paper_aff_dict = meta_data.paperAff.iloc[i]\n",
    "    \n",
    "    for a_info in authors_info:\n",
    "        ## 作者id就是count顺序编号\n",
    "        aname = a_info[0].lower().strip().replace('_','')\n",
    "        if aname[0] in set(['\\n', '\\u2008', '<', '\\xa0', '.', '[', '(']):\n",
    "            test+=1\n",
    "            break\n",
    "        aid.append(set([str(count)]))\n",
    "        gid.append(str(count))\n",
    "        aname_fix = re.sub(u\"\\\\(.*\\\\)|\\\\{.*}|\\\\[.*]\", \"\", aname).replace(', jr.','').replace(' jr.','').strip()\n",
    "        author_name.append(aname_fix) #作者名字\n",
    "        alter_name.append(set([aname_fix])) #同义的作者名字\n",
    "        raw_name.append(a_info[0])\n",
    "        sp_list = aname_fix.split()\n",
    "        name_split.append(sp_list)\n",
    "        first_letter.append(aname_fix[0])\n",
    "        last_name.append(sp_list[-1])\n",
    "        \n",
    "        aff_name = []\n",
    "        for i in a_info[1]:\n",
    "            if i in paper_aff_dict:\n",
    "                aff_name.append(paper_aff_dict[i].lower()) #机构名字的字符，而不是代号# 可能存在作者机构在文章机构中没有记载的情况，这种情况下作者机构为空\n",
    "        author_aff.append(set(aff_name)) #一个作者可能有多个机构\n",
    "        \n",
    "        doi.append(set([p_doi]))\n",
    "        journal.append(set([re.sub('[\\d,./]', '', p_doi)]))\n",
    "        ##被哪些doi引用了，用于互引，所以不是一篇论文没有关系\n",
    "        if p_doi in reference_dict.keys():\n",
    "            ref_set.append(reference_dict[p_doi])\n",
    "        else:\n",
    "            ref_set.append(set([]))\n",
    "        ## 合作作者id为这篇paper的其他作者id\n",
    "        coauthor_set.append(set(pap_aids)-set([str(count)]))\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661a1c8a-275f-4193-b114-5be3f60eadab",
   "metadata": {},
   "outputs": [],
   "source": [
    "author_paper = pd.DataFrame({\n",
    "    'gid':gid,\n",
    "    'aid':aid,\n",
    "    'rawName':raw_name,\n",
    "    'authorName':author_name,\n",
    "    'firstLetter':first_letter,\n",
    "    'lastName':last_name,\n",
    "    'alterName':alter_name,\n",
    "    'nameSplit':name_split,\n",
    "    'doi':doi,\n",
    "    'journal':journal,\n",
    "    'refSet':ref_set,\n",
    "    'coauthorSet':coauthor_set,\n",
    "    'authorAff':author_aff,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c0ab9d-4c75-4490-bf39-0abfaee78c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_group = {}\n",
    "for key, value in tqdm.tqdm(author_paper.groupby(['firstLetter','lastName'])):\n",
    "    sim_group[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b90923-657b-45ca-93d3-ded5433ba1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "global aff_names # 大大的string\n",
    "aff_names = ''\n",
    "for i in tqdm.tqdm(range(len(meta_data))):\n",
    "    try:\n",
    "        for j in list(meta_data['paperAff'].iloc[i].values()):\n",
    "            term_list = j.replace(',','').replace('.','').lower().split()\n",
    "            term_str = ' '.join(list(set(term_list)))\n",
    "            aff_names+=term_str\n",
    "            aff_names+=' '\n",
    "    except:\n",
    "        continue # 为空的情况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca69bdb-3600-4d37-93e2-44cd175bf0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def termFrequency(term, document):\n",
    "#     print(document.count(term),float(len(document)))\n",
    "    return document.count(term) / float(len(document))\n",
    "\n",
    "def computeTf(document):\n",
    "    sentence = document.replace(',','').replace('.','').lower().split()\n",
    "    tf= dict.fromkeys(set(sentence), 0)\n",
    "    for word in sentence:\n",
    "        tf[word] = termFrequency(word, sentence)\n",
    "    return tf\n",
    "\n",
    "def inverseDocumentFrequency(term, documents): \n",
    "    global idf_dict\n",
    "    if term in idf_dict.keys():\n",
    "        df = idf_dict[term]\n",
    "    else:\n",
    "        df = documents.count(term)\n",
    "        idf_dict[term] = df\n",
    "    return math.log(float(1348385) / df)\n",
    "    \n",
    "def computeIdf(document, documents):\n",
    "    idf_dict = {}\n",
    "    sentence = document.replace(',','').replace('.','').lower().split()\n",
    "    for word in sentence:\n",
    "        idf_dict[word] = inverseDocumentFrequency(word, documents)\n",
    "    return idf_dict\n",
    "\n",
    "def tfIdf(aff_name, aff_names):\n",
    "    vec = {}\n",
    "    vec_tf = computeTf(aff_name)\n",
    "    vec_idf = computeIdf(aff_name, aff_names)\n",
    "    for key in vec_tf.keys():\n",
    "        vec[key] = vec_tf[key]*vec_idf[key]\n",
    "    return vec\n",
    "\n",
    "def calSim(aff_name1, aff_name2, aff_names):\n",
    "    global idf_dict\n",
    "    \n",
    "    tf_idf_1 = tfIdf(aff_name1, aff_names)\n",
    "    tf_idf_2 = tfIdf(aff_name2, aff_names)\n",
    "    vec1 = []\n",
    "    vec2 = []\n",
    "#     print(set(tf_idf_1.keys()).union(set(tf_idf_2.keys())))\n",
    "    for key in (set(tf_idf_1.keys()).union(set(tf_idf_2.keys()))):\n",
    "        if key in tf_idf_1.keys():\n",
    "            vec1.append(tf_idf_1[key])\n",
    "        else:\n",
    "            vec1.append(0)\n",
    "        if key in tf_idf_2.keys():\n",
    "            vec2.append(tf_idf_2[key])\n",
    "        else:\n",
    "            vec2.append(0)\n",
    "            \n",
    "    vec1 = np.array(vec1)\n",
    "    vec2 = np.array(vec2)\n",
    "    sim = vec1.dot(vec2)/(np.linalg.norm(vec1)*np.linalg.norm(vec2))\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740d45ac-c672-4f30-9290-b685a30d498d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def isSameNameSet(nameset1, nameset2):\n",
    "    flag = '2'\n",
    "    for name1 in nameset1:\n",
    "        for name2 in nameset2:\n",
    "            if isSameName(name1, name2):\n",
    "                if ('1' in isSameName(name1, name2)):#全名相同\n",
    "#                     print('1', name1, name2)\n",
    "                    return '1'\n",
    "    for name1 in nameset1:\n",
    "        for name2 in nameset2:\n",
    "            if not bool(isSameName(name1, name2)):\n",
    "                return False\n",
    "    return flag\n",
    "def isSameName(name1, name2):\n",
    "    global aisian_names\n",
    "    '''\n",
    "    same: return True\n",
    "    different: return False\n",
    "    '''\n",
    "    #姓氏和首字已经相同\n",
    "    name1_list = re.findall(r'[^\\-\\s]+', name1.replace('.','. '))\n",
    "    name2_list = re.findall(r'[^\\-\\s]+', name2.replace('.','. '))\n",
    "    #名字\n",
    "    flag = '1' #全名相同\n",
    "    if (len(name1_list) == len(name2_list)):\n",
    "        for i in range(len(name1_list)):\n",
    "            part1 = name1_list[i]\n",
    "            part2 = name2_list[i]\n",
    "            if ('.' not in part1)&('.' not in part2):\n",
    "                if part1!=part2: ## 没缩写的话，名字相同\n",
    "                    return False  \n",
    "            else:## 有缩写的话，首字母相同\n",
    "                if part1[0]!=part2[0]:\n",
    "                    return False\n",
    "                else:\n",
    "                    flag = '2'\n",
    "#                 print(flag)\n",
    "    else:\n",
    "        flag = '2' #这种情况下只能是相容\n",
    "        for i in range(min(len(name1_list), len(name2_list))):\n",
    "            part1 = name1_list[i]\n",
    "            part2 = name2_list[i]\n",
    "            if ('.' not in part1)&('.' not in part2):\n",
    "                if part1!=part2: ## 没缩写的话，名字相同\n",
    "                    return False  \n",
    "            else:## 有缩写的话，首字母相同\n",
    "                if part1[0]!=part2[0]:\n",
    "                    return False\n",
    "    return flag\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c76798-16a5-43d0-9f5d-7248bf8e1db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ifSameAff():\n",
    "    global aff_names, aname_list, aid_list, gid_list, doi_list, refSet_list, coauthorSet_list, authorAff_list, co_update_dict, journal_list, pair_set_dict\n",
    " \n",
    "    i = 0\n",
    "    while i<len(aid_list):\n",
    "        stop_set = []\n",
    "\n",
    "        for j in range(i+1, len(aid_list)):                                    \n",
    "                ## 如果两个人有多个机构，则只要其中一对机构满足相似性要求就判定为相似                    \n",
    "            if isSameNameSet(aname_list[i], aname_list[j]):\n",
    "\n",
    "                if authorAff_list[i] & authorAff_list[j]:\n",
    "                    update_list(i,j)\n",
    "                    stop_set.append(j)   \n",
    "\n",
    "                \n",
    "        del_list(stop_set)\n",
    "        \n",
    "        i+=1 # 除掉stopset/下一个人\n",
    "    \n",
    "    return True\n",
    "\n",
    "def ifSameJournal():\n",
    "    global aff_names, aname_list, aid_list, gid_list, doi_list, refSet_list, coauthorSet_list, authorAff_list, co_update_dict, journal_list, pair_set_dict\n",
    " \n",
    "    i = 0\n",
    "    while i<len(aid_list):\n",
    "        stop_set = []\n",
    "\n",
    "        for j in range(i+1, len(aid_list)):                            \n",
    "                ## 如果两个人有多个机构，则只要其中一对机构满足相似性要求就判定为相似\n",
    "            if isSameNameSet(aname_list[i], aname_list[j]):\n",
    "\n",
    "                if bool(journal_list[i] & journal_list[j]):\n",
    "                    update_list(i,j)                                                              \n",
    "                    stop_set.append(j)   \n",
    "\n",
    "        del_list(stop_set)\n",
    "        \n",
    "        i+=1 # 除掉stopset/下一个人\n",
    "    \n",
    "    return True\n",
    "\n",
    "def ifSimilar(sim_thres):\n",
    "    global idf_dict, aff_names, aname_list, aid_list, gid_list, doi_list, refSet_list, coauthorSet_list, authorAff_list, co_update_dict, journal_list, pair_set_dict\n",
    " \n",
    "    i = 0\n",
    "    while i<len(aid_list):\n",
    "\n",
    "        stop_set = []\n",
    "    \n",
    "        for j in range(i+1, len(aid_list)):\n",
    "\n",
    "            if isSameNameSet(aname_list[i], aname_list[j]):\n",
    "\n",
    "                sims = []\n",
    "                for aff_name1 in authorAff_list[i]:\n",
    "                    for aff_name2 in authorAff_list[j]:\n",
    "                        sims.append(calSim(aff_name1, aff_name2, aff_names))\n",
    "                    \n",
    "                if len(sims)!=0:\n",
    "                    if max(sims) >= sim_thres:\n",
    "                        update_list(i,j)\n",
    "                        stop_set.append(j)\n",
    "        \n",
    "        del_list(stop_set)\n",
    "        \n",
    "        i+=1\n",
    "    \n",
    "    return True\n",
    "\n",
    "def ifCoauthor(): #共引更新！\n",
    "\n",
    "    global aid_list, aname_list, gid_list, doi_list, refSet_list, coauthorSet_list, authorAff_list, co_update_dict, journal_list, pair_set_dict\n",
    " \n",
    "    i = 0\n",
    "    while i<len(aid_list):\n",
    "        stop_set = []\n",
    "        for j in range(i+1, len(aid_list)):\n",
    "                                    \n",
    "            if isSameNameSet(aname_list[i], aname_list[j]):\n",
    "                    \n",
    "                if coauthorSet_list[i] & coauthorSet_list[j]: ## 如果两个人有共同的coauthor_list\n",
    "\n",
    "                    update_list(i,j)\n",
    "                    stop_set.append(j)\n",
    "        \n",
    "        del_list(stop_set)\n",
    "        \n",
    "        i+=1\n",
    "    \n",
    "    return True\n",
    "\n",
    "def ifrefEachOther():\n",
    "    global aid_list, aname_list, gid_list, doi_list, refSet_list, coauthorSet_list, authorAff_list, co_update_dict, journal_list, pair_set_dict\n",
    "\n",
    "    i = 0\n",
    "    while i<len(aid_list):\n",
    "\n",
    "        stop_set = []\n",
    "        \n",
    "        for j in range(i+1, len(aid_list)):\n",
    "            \n",
    "            if isSameNameSet(aname_list[i], aname_list[j]):\n",
    "                \n",
    "                if (doi_list[j] & refSet_list[i]) & (doi_list[i] & refSet_list[j]):\n",
    "                    update_list(i,j)\n",
    "                    stop_set.append(j)\n",
    "        del_list(stop_set)\n",
    "    \n",
    "        i+=1\n",
    "    return True\n",
    "\n",
    "def ifSameWholeName():\n",
    "    global aid_list, aname_list, gid_list, doi_list, refSet_list, coauthorSet_list, authorAff_list, co_update_dict, journal_list, pair_set_dict\n",
    "\n",
    "    i = 0\n",
    "    while i<len(aid_list):\n",
    "\n",
    "        stop_set = []\n",
    "        \n",
    "        for j in range(i+1, len(aid_list)):\n",
    "            \n",
    "            if isSameNameSet(aname_list[i], aname_list[j])=='1':\n",
    "\n",
    "                update_list(i,j)\n",
    "                stop_set.append(j)\n",
    "                    \n",
    "        del_list(stop_set)\n",
    "    \n",
    "        i+=1\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f65d72f-5daa-40b4-9508-b22a8d87eacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_list(i, j):\n",
    "    global aid_list, aname_list, gid_list, doi_list, refSet_list, coauthorSet_list, authorAff_list, co_update_dict, journal_list\n",
    "    aid_list[i]=aid_list[i]|aid_list[j]\n",
    "    aname_list[i]=aname_list[i]|aname_list[j]\n",
    "    doi_list[i]=doi_list[i]|doi_list[j]\n",
    "    refSet_list[i]=refSet_list[i]|refSet_list[j]\n",
    "                \n",
    "#   update_author\n",
    "    if gid_list[j] in co_update_dict.keys():\n",
    "        co_update_dict[gid_list[j]].update(aid_list[i])\n",
    "    else:\n",
    "        co_update_dict[gid_list[j]] = aid_list[i]\n",
    "    coauthorSet_list[i]=coauthorSet_list[i]|coauthorSet_list[j]\n",
    "    authorAff_list[i]=authorAff_list[i]|authorAff_list[j]\n",
    "    journal_list[i] = journal_list[i]|journal_list[j]\n",
    "    \n",
    "\n",
    "def del_list(stop_set):\n",
    "    global aid_list, aname_list, gid_list, doi_list, refSet_list, coauthorSet_list, authorAff_list, co_update_dict, journal_list\n",
    "    aid_list = [aid_list[k] for k in range(0, len(aid_list), 1) if k not in stop_set]\n",
    "    aname_list = [aname_list[k] for k in range(0, len(aname_list), 1) if k not in stop_set]\n",
    "    gid_list = [gid_list[k] for k in range(0, len(gid_list), 1) if k not in stop_set]\n",
    "    doi_list = [doi_list[k] for k in range(0, len(doi_list), 1) if k not in stop_set]\n",
    "    refSet_list = [refSet_list[k] for k in range(0, len(refSet_list), 1) if k not in stop_set]\n",
    "    coauthorSet_list = [coauthorSet_list[k] for k in range(0, len(coauthorSet_list), 1) if k not in stop_set]\n",
    "    authorAff_list = [authorAff_list[k] for k in range(0, len(authorAff_list), 1) if k not in stop_set]  \n",
    "    journal_list = [journal_list[k] for k in range(0, len(journal_list), 1) if k not in stop_set]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984df034-81e2-4d89-8005-d8af92287c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "global co_update_dict, idf_dict, pair_set_dict\n",
    "idf_dict = {}\n",
    "co_update_dict = {}\n",
    "pair_set_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7f35c4-26a3-4d6f-96d9-dd6caf92c84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "aid=[]\n",
    "aname = []\n",
    "gid=[]\n",
    "doi=[]\n",
    "refSet=[]\n",
    "coauthorSet=[]\n",
    "authorAff=[]\n",
    "\n",
    "SIM_THRESHOLD = 0.15\n",
    "count = 0\n",
    "\n",
    "loop_group = {}\n",
    "\n",
    "for key in tqdm.tqdm(sim_group.keys()):\n",
    "           \n",
    "    df = sim_group[key]\n",
    "    \n",
    "    if count%20000 == 0 :\n",
    "        print('key=',key)\n",
    "\n",
    "    global aid_list, aname_list, gid_list, doi_list, refSet_list, coauthorSet_list, authorAff_list, journal_list\n",
    "    ## 每个组里动态更新\n",
    "    aid_list=list(df.aid)\n",
    "    aname_list=list(df.alterName)\n",
    "    gid_list=list(df.gid)\n",
    "    doi_list = list(df.doi)\n",
    "    refSet_list = list(df.refSet)\n",
    "    coauthorSet_list = list(df.coauthorSet)\n",
    "    journal_list= list(df.journal)\n",
    "\n",
    "    for idx in range(len(coauthorSet_list)):\n",
    "        if coauthorSet_list[idx]&co_update_dict.keys(): ##如果这个集合里面有字典里的元素\n",
    "            for item in coauthorSet_list[idx]&co_update_dict.keys():\n",
    "                coauthorSet_list[idx].update(co_update_dict[item])\n",
    "\n",
    "    authorAff_list = list(df.authorAff)\n",
    "\n",
    "    if len(df)!=1: ## 组里只有一个人：直接跳过\n",
    "\n",
    "        loop = True\n",
    "        while loop:\n",
    "            df_length =len(aid_list)\n",
    "            \n",
    "            ifSameAff() \n",
    "            ifSimilar(0.15)\n",
    "            \n",
    "            ifrefEachOther() #名字相似且符合条件1的人，认为是同一个人，合并\n",
    "            ifCoauthor()\n",
    "                        # df一直在更新\n",
    "            if (len(aid_list)==df_length):\n",
    "                    # 什么时候停止循环？一个循环下来，len(df)不变\n",
    "                loop = False\n",
    "        \n",
    "        ifSameWholeName()\n",
    "        ifSameJournal()\n",
    "        \n",
    "        loop = True\n",
    "        while loop:\n",
    "            df_length =len(aid_list)\n",
    "\n",
    "            ifrefEachOther() #名字相似且符合条件1的人，认为是同一个人，合并\n",
    "            ifCoauthor()\n",
    "                        # df一直在更新\n",
    "            if (len(aid_list)==df_length):\n",
    "                    # 什么时候停止循环？一个循环下来，len(df)不变\n",
    "                loop = False\n",
    "                \n",
    "    aid+=aid_list\n",
    "    aname+=aname_list\n",
    "    gid+=gid_list\n",
    "    doi+=doi_list\n",
    "    \n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137e9437-f40f-4608-b2c8-09b59b95212e",
   "metadata": {},
   "outputs": [],
   "source": [
    "author_paper_2 = pd.DataFrame({\n",
    "    'aid':aid,\n",
    "    'aname':aname,\n",
    "    'gid':gid,\n",
    "    'doi':doi,\n",
    "})\n",
    "author_paper_2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
