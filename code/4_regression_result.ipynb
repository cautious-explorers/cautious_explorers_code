{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8368dcb-5e5b-4c1c-b30b-fe5ed6c01b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def save_pkl(path,obj):\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(obj,f)\n",
    "        \n",
    "def load_pkl(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        return pickle.load(f) \n",
    "\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import tqdm as tq\n",
    "import datetime\n",
    "from scipy import stats\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import scipy.stats as st\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30143fa9-2b28-4778-97cc-3303546a42ab",
   "metadata": {},
   "source": [
    "# split_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0fb4f989-3200-4ffb-885c-481702dc1769",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_paper(file_dir1, file_dir2, save_dir, behave):\n",
    "    for cut_paper in tq.tqdm(range(10,11)):\n",
    "        data = pd.read_csv(file_dir1+'original_aps.csv')\n",
    "        data = data[data.paperCount>cut_paper] #split point\n",
    "        data['next_year']=data.date.astype('str').str[:4]# year\n",
    "        \n",
    "        data['genres'] = [[int(gg[:2]) for gg in g[1:-1].replace('\\'', '').replace(' ', '').split(',')] for g in data['genres']]\n",
    "            \n",
    "        data.next_year=data.next_year.astype('int')\n",
    "        data.sort_values(['aid','next_year'],inplace=True)\n",
    "        data['pre_count'] = data.groupby('aid').date.rank(method='first') # sort data by paper publication order\n",
    "        datak1 = data[data.attempt_number <=cut_paper].groupby(['aid']) # data before split point\n",
    "        datak2 = data[data.attempt_number >cut_paper].groupby(['aid']) # data after split point\n",
    "\n",
    "        N= datak1.paperDoi.count()# the number of papers before split point\n",
    "        N= N.reset_index()\n",
    "        first_year=datak1.next_year.head(1)# first year\n",
    "        N['first_year']=list( first_year )\n",
    "        first_gere=datak1.genres.head(1)#first area\n",
    "        N['first_genre']=list( first_gere )\n",
    "        pre_for =  datak1[behave].mean()# performance before split point\n",
    "        N['past_logCit']= list(pre_for)\n",
    "\n",
    "        next_for = datak2[behave].mean()\n",
    "        next_for = next_for.reset_index()\n",
    "        next_count = datak2[behave].count()\n",
    "        next_for['post_paperCount'] = list(next_count)\n",
    "        whole_career = datak2['CareerYear'].apply(lambda x: x.iloc[-1]-x.iloc[0])\n",
    "        next_for['post_career'] = list(whole_career)\n",
    "        next_for['whole_count'] = list(datak2['paperCount'].tail(1))\n",
    "\n",
    "        N =N.merge(next_for,on=['aid'],how='left')\n",
    "\n",
    "        #explotary metrics\n",
    "        switch=pd.read_csv(file_dir2+'switch.csv')\n",
    "        switch.aid=switch.aid.astype(int)\n",
    "        switch = switch[switch.aid.isin(data.aid)]\n",
    "        switch.rename(columns={'N5_es_distance':'es_distance'},inplace=True)\n",
    "        switch.rename(columns={'N5_es':'es'},inplace=True)\n",
    "        switch.es=switch.es.astype(int)\n",
    "        switch['next_year']=switch.thisDate.astype('str').str[:4]\n",
    "        switch.next_year=switch.next_year.astype('int')\n",
    "        switch.sort_values(['aid','next_year'],inplace=True)\n",
    "        pre_switch = switch[switch.attempt_number <= cut_paper].groupby(['aid'])\n",
    "\n",
    "        switch2=pre_switch.es.mean()\n",
    "        es_distance = pre_switch.es_distance.mean()\n",
    "        switch2=switch2.reset_index()\n",
    "        switch2.aid=switch2.aid.astype('int')\n",
    "        switch2['past_es_dis'] = list(es_distance)\n",
    "\n",
    "        N=N.merge(switch2,on=['aid']) #merge data\n",
    "\n",
    "        \n",
    "        N.rename(columns={'paperDoi':'past_paperCount',str(behave):'post_logCit','es':'past_es'},inplace=True)\n",
    "        N.to_csv(save_dir+\"split_paper/N_{}_author_info.csv\".format(cut_paper), index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d0e03e68-689d-4a8e-87de-7d9acedea47d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:07<00:00,  7.34s/it]\n"
     ]
    }
   ],
   "source": [
    "cut_paper('../data/regression/','../data/regression/','../data/regression/','logCit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ad719895-b626-475a-b447-7eb991878c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_career(file_dir1, file_dir2, save_dir, behave):\n",
    "    for cut_paper in tq.tqdm(range(10,11)):\n",
    "        data = pd.read_csv(file_dir1+'original_aps.csv')\n",
    "        data = data[data.cyCount>cut_paper] \n",
    "        data['next_year']=data.date.astype('str').str[:4]\n",
    "        data.next_year=data.next_year.astype('int')\n",
    "        data.sort_values(['aid','next_year'],inplace=True)\n",
    "        \n",
    "        data['genres'] = [[int(gg[:2]) for gg in g[1:-1].replace('\\'', '').replace(' ', '').split(',')] for g in data['genres']]\n",
    "\n",
    "        \n",
    "        datak1 = data[data.CareerYear <=cut_paper].groupby(['aid']) \n",
    "        datak2 = data[data.CareerYear >cut_paper].groupby(['aid'])\n",
    "\n",
    "        N= datak1.paperDoi.count()\n",
    "        N= N.reset_index()\n",
    "        first_year=datak1.next_year.head(1)\n",
    "        N['first_year']=list( first_year )\n",
    "        first_gere=datak1.genres.head(1)\n",
    "        N['first_genre']=list( first_gere )\n",
    "        pre_for =  datak1[behave].mean()\n",
    "        N['past_logCit']= list(pre_for)\n",
    "\n",
    "        next_for = datak2[behave].mean()\n",
    "        next_for = next_for.reset_index()\n",
    "        next_count = datak2[behave].count()\n",
    "        next_for['post_paperCount'] = list(next_count)\n",
    "        whole_career = datak2['CareerYear'].apply(lambda x: x.iloc[-1]-x.iloc[0])\n",
    "        next_for['post_career'] = list(whole_career)\n",
    "        next_for['whole_count'] = list(datak2['paperCount'].tail(1))\n",
    "\n",
    "        N =N.merge(next_for,on=['aid'],how='left')\n",
    "\n",
    "\n",
    "        switch=pd.read_csv(file_dir2+'switch.csv')\n",
    "        switch.aid=switch.aid.astype(int)\n",
    "        switch = switch[switch.aid.isin(data.aid)]\n",
    "        switch.rename(columns={'N5_es_distance':'es_distance'},inplace=True)\n",
    "        switch.rename(columns={'N5_es':'es'},inplace=True)\n",
    "        switch.es=switch.es.astype(int)\n",
    "        switch['next_year']=switch.thisDate.astype('str').str[:4]\n",
    "        switch.next_year=switch.next_year.astype('int')\n",
    "        switch.sort_values(['aid','next_year'],inplace=True)\n",
    "        pre_switch = switch[switch.CareerYear <=cut_paper].groupby(['aid'])\n",
    "        switch2=pre_switch.es.mean()\n",
    "        es_distance = pre_switch.es_distance.mean()\n",
    "        switch2=switch2.reset_index()\n",
    "        switch2.aid=switch2.aid.astype('int')\n",
    "        switch2['past_es_dis'] = list(es_distance)\n",
    "\n",
    "        N=N.merge(switch2,on=['aid']) \n",
    "        \n",
    "        N.rename(columns={'paperDoi':'past_paperCount',str(behave):'post_logCit','es':'past_es'},inplace=True)\n",
    "        N.to_csv(save_dir+\"split_career/cyear_{}_author_info.csv\".format(cut_paper), index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f14ea602-68e2-4c4a-85ef-2c39c6ddcdb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:06<00:00,  6.58s/it]\n"
     ]
    }
   ],
   "source": [
    "cut_career('../data/regression/','../data/regression/','../data/regression/','logCit')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0bcd072-0467-40be-bb64-63a2c957ff6b",
   "metadata": {},
   "source": [
    "# regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "079e8d64-2b1d-4192-8dee-be981ec0d83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dummay variable for paper field\n",
    "def get_dummy(datak):\n",
    "    list100=[]\n",
    "    aidlist=[]\n",
    "    for i,g in datak.groupby(['aid']):\n",
    "        aidlist.append(i)\n",
    "        \n",
    "        this_genres = [np.nan]*100\n",
    "        i=list(g.first_genre)[0]\n",
    "        i=i[1:-1]\n",
    "        i=i.replace('\\'', '') \n",
    "        i=i.replace(' ', '') \n",
    "        i=i.split(',')\n",
    "        for t_i in i:\n",
    "            this_genres[int(t_i[:2])]=1\n",
    "        list100.append(this_genres)\n",
    "    \n",
    "    aid_genre=pd.DataFrame( list100  )\n",
    "    aid_genre['aid'] = aidlist\n",
    "    datak=datak.merge(aid_genre,on=['aid']).drop(columns='first_genre')\n",
    "    return datak\n",
    "\n",
    "\n",
    "def cal_norm_k(k, std_x, std_y):\n",
    "    return k * std_x / std_y\n",
    "\n",
    "def draw_relation_explore_score_each(attribute_x, attribute_y1, attribute_y2, width, till_rate, data):\n",
    "    director_count = []\n",
    "    strictly_switch_prob_mean_1 = []\n",
    "    strictly_switch_prob_mean_2 = []\n",
    "    strictly_switch_prob_std = []\n",
    "    low_list_1 = []\n",
    "    high_list_1 = []\n",
    "    low_list_2 = []\n",
    "    high_list_2 = []\n",
    "\n",
    "    x_labels = np.arange(data[attribute_x].min(), data[attribute_x].max(), width)\n",
    "\n",
    "    for lower_rating in x_labels:\n",
    "\n",
    "        higher_rating = lower_rating + width\n",
    "\n",
    "        required_dir = data.loc[(data[attribute_x] >= lower_rating) & (data[attribute_x] < higher_rating)]\n",
    "        director_count.append(len(required_dir))\n",
    "        strictly_switch_prob_mean_1.append(required_dir[attribute_y1].mean())\n",
    "    \n",
    "        strictly_switch_prob_mean_2.append(required_dir[attribute_y2].mean())\n",
    "\n",
    "    fig=plt.figure(figsize=(5, 3))\n",
    "    ax1=fig.add_subplot(111)\n",
    "\n",
    "    till=int(len(x_labels)*till_rate)\n",
    "    line1 = ax1.scatter(x_labels[:till], strictly_switch_prob_mean_1[:till], label=attribute_y1, color = 'r')\n",
    "    line2 = ax1.plot(x_labels[:till], strictly_switch_prob_mean_2[:till], label=attribute_y2, color = 'g')\n",
    "    ax1.legend(loc='best')\n",
    "    plt.xlabel(attribute_x)\n",
    "    ax1.set_ylabel(attribute_y1)\n",
    "\n",
    "    ax2=ax1.twinx()\n",
    "    ax2.bar(x_labels[:till], director_count[:till], label='Count', alpha=0.2, width = width)\n",
    "    ax2.set_ylabel('Count')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def mape(y_true, y_pred): \n",
    "    return np.mean(np.abs((y_pred - y_true) / [max(i,0.00000001) for i in y_true])) \n",
    "def cal_norm_k(k, std_x, std_y):\n",
    "    return k * std_x / std_y\n",
    "\n",
    "\n",
    "def draw_distribution(attribute1, attribute2, width, till_rate, data):\n",
    "    fig=plt.figure(figsize=(5, 3))\n",
    "    ax1=fig.add_subplot(111)\n",
    "    \n",
    "\n",
    "    for attr in [attribute1, attribute2]:\n",
    "        director_count = []\n",
    "        x_labels = np.arange(data[attr].min(), data[attr].max(), width)\n",
    "\n",
    "        for lower_rating in x_labels:\n",
    "\n",
    "            higher_rating = lower_rating + width\n",
    "            required_dir = data[(data[attr] >= lower_rating) & (data[attr] < higher_rating)]\n",
    "            director_count.append(len(required_dir))\n",
    "\n",
    "        till=int(len(x_labels)*till_rate)\n",
    "        ax1.bar(x_labels[:till], director_count[:till], label=attr, alpha=0.2, width = width)\n",
    "    \n",
    "    ax1.set_ylabel('Count')\n",
    "    plt.xlabel(attribute1)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def cal_norm_k(k, std_x, std_y):\n",
    "    return k * std_x / std_y\n",
    "\n",
    "\n",
    "def reg_and_pre(poy, file_dir, select, attribute_y, attributes, dummy_attris, summary,range_list):\n",
    "    N = []\n",
    "    r2 = []\n",
    "    mse = []\n",
    "    rmse = []\n",
    "    mae = []\n",
    "\n",
    "    mu_bar = []\n",
    "    mu = []\n",
    "    std_bar = []\n",
    "    std = []\n",
    "    \n",
    "    pvalue = {}\n",
    "    norm_k_order = []\n",
    "    coeff = {}\n",
    "    norm_coeff = {}\n",
    "    err = {}\n",
    "    norm_err = {}\n",
    "\n",
    "    if poy == 'p':\n",
    "        f_str = \"N_{}_author_info.csv\"\n",
    "        \n",
    "    elif poy == 'cy':\n",
    "        f_str = \"cyear_{}_author_info.csv\"\n",
    "\n",
    "    for cut_year in range_list:\n",
    "        \n",
    "        predict=pd.read_csv(file_dir+f_str.format(cut_year))\n",
    "        \n",
    "        if select:\n",
    "            if poy == 'y':\n",
    "                predict = predict[(predict.past_paperCount>=5)&(predict.post_paperCount>=3)]\n",
    "                behave_list = ['past_logCit','past_paperCount']\n",
    "            if poy == 'p':\n",
    "                predict = predict[(predict.post_paperCount>=3)]\n",
    "                behave_list = ['past_logCit']\n",
    "            if poy == 'cy':\n",
    "                predict = predict[(predict.past_paperCount>=5)&(predict.post_paperCount>=3)]\n",
    "                behave_list = ['past_logCit','past_paperCount']\n",
    "        \n",
    "        # print(len(predict))\n",
    "\n",
    "        if attributes:\n",
    "            x_label_1 = attributes + behave_list\n",
    "            \n",
    "        else:\n",
    "            x_label_1 = behave_list\n",
    "            \n",
    "        predict = predict[x_label_1+['aid','first_genre','first_year']+[attribute_y]]  \n",
    "        predict = predict.dropna()\n",
    "        if len(predict)==0:\n",
    "            continue\n",
    "            \n",
    "        predict = get_dummy(predict)\n",
    "        predict = pd.concat((predict, pd.get_dummies(predict['first_year'], drop_first=True)), axis=1).drop(columns='first_year')\n",
    "        if attributes:\n",
    "            attributes_used = attributes.copy()\n",
    "        if dummy_attris:\n",
    "            for attri in dummy_attris:\n",
    "                dummy_df = pd.get_dummies(predict[attri]).drop(columns=max(predict[attri]))\n",
    "                predict = pd.concat((predict, dummy_df), axis=1).drop(columns=attri)\n",
    "                attributes_used += list(dummy_df.columns)\n",
    "                attributes_used.remove(attri)\n",
    "        \n",
    "        # print(attributes)\n",
    "        predict=predict.dropna(axis=1,how='all')\n",
    "        predict.fillna(0,inplace=True)\n",
    "        # print(predict.describe())\n",
    "        \n",
    "        x_label = list(set(predict.columns) - set(['aid',attribute_y]))\n",
    "        # print(x_label)\n",
    "        X_train=predict[x_label]\n",
    "        X_train=sm.add_constant(X_train)\n",
    "        Y_train=predict[attribute_y]\n",
    "        # print(X_train[['A','B','C']].head())\n",
    "        # print(np.asarray(X_train))\n",
    "        est = sm.OLS(Y_train , X_train).fit()\n",
    "        \n",
    "        \n",
    "        if summary:\n",
    "            print(est.summary())\n",
    "        \n",
    "        if attributes:\n",
    "            for a in attributes_used:\n",
    "                if a not in coeff.keys():\n",
    "                    pvalue[a] = [float(est.pvalues[a])]\n",
    "                    coeff[a] = [float(est.params[a])]\n",
    "                    norm_coeff[a] = [cal_norm_k(float(est.params[a]), X_train[a].std(), Y_train.std())]\n",
    "                    err[a] = [float(est.bse[a])]\n",
    "                    norm_err[a] = [cal_norm_k(float(est.bse[a]), X_train[a].std(), Y_train.std())]\n",
    "                else:\n",
    "                    pvalue[a].append(float(est.pvalues[a]))\n",
    "                    coeff[a].append(float(est.params[a]))\n",
    "                    norm_coeff[a].append(cal_norm_k(float(est.params[a]), X_train[a].std(), Y_train.std()))\n",
    "                    err[a].append(float(est.bse[a]))\n",
    "                    norm_err[a].append(cal_norm_k(float(est.bse[a]), X_train[a].std(), Y_train.std()))\n",
    "        N.append(est.nobs)    \n",
    "        r2.append(est.rsquared_adj)\n",
    "    \n",
    "    parameters = {'r2':r2, 'N':N, 'mse':mse, 'rmse':rmse, 'mae':mae, 'mu_bar':mu_bar, 'mu':mu, 'std_bar':std_bar, 'std':std, 'pvalue':pvalue, 'norm_k_order':norm_k_order,'coeff':coeff,'norm_coeff':norm_coeff, 'err':err, 'norm_err':norm_err}\n",
    "    print(parameters.keys())\n",
    "    \n",
    "    return parameters, est\n",
    "\n",
    "\n",
    "def test(para):\n",
    "    r2, N, mse, rmse, mae, mu_bar, mu, std_bar, std, pvalue, norm_k_order,coeff,norm_coeff,err, norm_err = para[0].values()\n",
    "    for key in coeff.keys():\n",
    "        print('%sThe regression coefficient are positive'%key,'%.2f'%(sum([c>0 for c in coeff[key]])/len(coeff[key])))\n",
    "        print('%sThe regression coefficient are significant, p value<=0.05'%key,'%.2f'%(sum([p<0.05 for p in pvalue[key]])/len(pvalue[key])))\n",
    "    print('Average predicted R2 %.2f ranges %.2f-%.2f'%(np.mean(r2),np.min(r2),np.max(r2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eb1ef6a8-a6a7-4b6d-9234-4cae3ae6fe71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['r2', 'N', 'mse', 'rmse', 'mae', 'mu_bar', 'mu', 'std_bar', 'std', 'pvalue', 'norm_k_order', 'coeff', 'norm_coeff', 'err', 'norm_err'])\n",
      "past_es回归系数为正 1.00\n",
      "past_es回归系数显著<=0.05 0.00\n",
      "past_es_dis回归系数为正 0.00\n",
      "past_es_dis回归系数显著<=0.05 1.00\n",
      "预测R2平均0.10范围0.10-0.10\n"
     ]
    }
   ],
   "source": [
    "file_dir ='../data/regression/split_paper/'\n",
    "\n",
    "para_1 = reg_and_pre('p', file_dir, True, 'post_paperCount', ['past_es','past_es_dis'], False, False, range(10,11))\n",
    "test(para_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1267ef0d-6dff-4fce-b0cd-469f73f1e4fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['r2', 'N', 'mse', 'rmse', 'mae', 'mu_bar', 'mu', 'std_bar', 'std', 'pvalue', 'norm_k_order', 'coeff', 'norm_coeff', 'err', 'norm_err'])\n",
      "past_es回归系数为正 0.00\n",
      "past_es回归系数显著<=0.05 1.00\n",
      "预测R2平均0.09范围0.09-0.09\n"
     ]
    }
   ],
   "source": [
    "para_2 = reg_and_pre('p', file_dir, True, 'post_paperCount', ['past_es'], False, False, range(10,11))\n",
    "test(para_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0e319470-60f7-4a9c-a07a-fe8d8972d084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['r2', 'N', 'mse', 'rmse', 'mae', 'mu_bar', 'mu', 'std_bar', 'std', 'pvalue', 'norm_k_order', 'coeff', 'norm_coeff', 'err', 'norm_err'])\n",
      "past_es回归系数为正 1.00\n",
      "past_es回归系数显著<=0.05 1.00\n",
      "past_es_dis回归系数为正 0.00\n",
      "past_es_dis回归系数显著<=0.05 0.00\n",
      "预测R2平均0.28范围0.28-0.28\n"
     ]
    }
   ],
   "source": [
    "file_dir ='../data/regression/split_career/'\n",
    "\n",
    "para_3 = reg_and_pre('cy', file_dir, True, 'post_paperCount', ['past_es','past_es_dis'], False, False, range(10,11))\n",
    "test(para_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eb29aae1-5449-4727-acd7-b2aac205f78c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['r2', 'N', 'mse', 'rmse', 'mae', 'mu_bar', 'mu', 'std_bar', 'std', 'pvalue', 'norm_k_order', 'coeff', 'norm_coeff', 'err', 'norm_err'])\n",
      "past_es回归系数为正 1.00\n",
      "past_es回归系数显著<=0.05 1.00\n",
      "预测R2平均0.28范围0.28-0.28\n"
     ]
    }
   ],
   "source": [
    "para_4 = reg_and_pre('cy', file_dir, True, 'post_paperCount', ['past_es'], False, False, range(10,11))\n",
    "test(para_4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mytensor-kernel",
   "language": "python",
   "name": "mytensor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
